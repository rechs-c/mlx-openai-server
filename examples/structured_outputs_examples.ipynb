{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX Server Structured Output Examples\n",
    "\n",
    "This is a detailed text version of the structured output examples for MLX Server with OpenAI-compatible API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client\n",
    "\n",
    "Connect to your local MLX server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url = \"http://localhost:8000/v1\",\n",
    "    api_key = \"mlx-server-api-key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the weather in Tokyo?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the available tools/functions\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the weather in a given city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\"type\": \"string\", \"description\": \"The city to get the weather for\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Streaming Function Calling Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl_1754135306120611', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1754135306725351', function=Function(arguments='{\"city\": \"Tokyo\"}', name='get_weather'), type='function', index=0)], reasoning_content=None))], created=1754135306, model='mlx-server-model', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None)\n"
     ]
    }
   ],
   "source": [
    "# Make the API call\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mlx-server-model\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    max_tokens = 512\n",
    ")\n",
    "\n",
    "# Get the result\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Function Calling Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_1754135307829795', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' {\"', name=None), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='city', name=None), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Tok', name=None), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='yo', name=None), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type='function')], reasoning_content=None), finish_reason=None, index=0, logprobs=None)], created=1754135306, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl_1754135306422307', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning_content=None), finish_reason='tool_calls', index=0, logprobs=None)], created=1754135308, model='mlx-server-model', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n"
     ]
    }
   ],
   "source": [
    "# Set stream=True in the API call\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mlx-server-model\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Process the streaming response\n",
    "for chunk in completion:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Schema Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Extract the address from the user input into the specified JSON format.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Please format this address: 1 Hacker Wy Menlo Park CA 94025\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"Address\",\n",
    "        \"schema\": {\n",
    "            \"properties\": {\n",
    "                \"address\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"street\": {\"type\": \"string\"},\n",
    "                    \"city\": {\"type\": \"string\"},\n",
    "                    \"state\": {\n",
    "                    \"type\": \"string\", \n",
    "                    \"description\": \"2 letter abbreviation of the state\"\n",
    "                    },\n",
    "                    \"zip\": {\n",
    "                    \"type\": \"string\", \n",
    "                    \"description\": \"5 digit zip code\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"street\", \"city\", \"state\", \"zip\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"address\"],\n",
    "            \"type\": \"object\"\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Structured Output Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl_1754135313793796', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"address\": {\"street\": \"1 Hacker Wy\", \"city\": \"Menlo Park\", \"state\": \"CA\", \"zip\": \"94025\"}}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None))], created=1754135313, model='mlx-server-model', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None)\n"
     ]
    }
   ],
   "source": [
    "# Make the API call\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mlx-server-model\",\n",
    "    messages=messages,\n",
    "    max_tokens = 512,\n",
    "    response_format = response_format\n",
    ")\n",
    "\n",
    "# Get the result\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Structured Output Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"address\": {\"street\": \"1 Hacker Wy\", \"city\": \"Menlo Park\", \"state\": \"CA\", \"zip\": \"94025\"}}"
     ]
    }
   ],
   "source": [
    "# Make the API call\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mlx-server-model\",\n",
    "    messages=messages,\n",
    "    max_tokens = 512,\n",
    "    response_format = response_format,\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "# Process the streaming response\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
